{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5107e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Subjects: 100%|██████████| 1030/1030 [12:42<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset creation complete.\n",
      "HDF5 files are saved in: /home/yaobo/Project/Lung-Cancer-Subtyping-Classification-V4.0/data/2_final_h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the base directories\n",
    "preprocessed_dir = '../../data/1_preprocessed'\n",
    "output_dir = '../../data/2_final_h5'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the list of subject directories\n",
    "subject_dirs = [d for d in os.listdir(preprocessed_dir) if os.path.isdir(os.path.join(preprocessed_dir, d)) and d.startswith('sub-')]\n",
    "\n",
    "# Process each subject\n",
    "for subject_dir in tqdm(subject_dirs, desc=\"Processing Subjects\"):\n",
    "    subject_id_with_prefix = os.path.basename(subject_dir)\n",
    "    subject_id = subject_id_with_prefix.replace('sub-', '')\n",
    "    \n",
    "    # Define the paths for the different modalities\n",
    "    ct_path = os.path.join(preprocessed_dir, subject_dir, f'{subject_id_with_prefix}_ct.nii.gz')\n",
    "    pet_path = os.path.join(preprocessed_dir, subject_dir, f'{subject_id_with_prefix}_pet_desc-suv.nii.gz')\n",
    "    mask_path = os.path.join(preprocessed_dir, subject_dir, f'{subject_id_with_prefix}_seg-lesion.nii.gz')\n",
    "    \n",
    "    # Check if all required files exist\n",
    "    if not all(os.path.exists(p) for p in [ct_path, pet_path, mask_path]):\n",
    "        print(f\"Skipping {subject_id}: Missing one or more required files.\")\n",
    "        continue\n",
    "        \n",
    "    # Define the output HDF5 file path\n",
    "    output_h5_path = os.path.join(output_dir, f'{subject_id}.h5')\n",
    "    \n",
    "    try:\n",
    "        # Create and write to the HDF5 file\n",
    "        with h5py.File(output_h5_path, 'w') as hf:\n",
    "            # Load and save CT data\n",
    "            ct_img = nib.load(ct_path)\n",
    "            ct_data = ct_img.get_fdata()\n",
    "            hf.create_dataset('CT', data=ct_data, compression=\"gzip\")\n",
    "            \n",
    "            # Load and save PET data\n",
    "            pet_img = nib.load(pet_path)\n",
    "            pet_data = pet_img.get_fdata()\n",
    "            hf.create_dataset('PET', data=pet_data, compression=\"gzip\")\n",
    "            \n",
    "            # Load and save Lesion Mask data\n",
    "            mask_img = nib.load(mask_path)\n",
    "            mask_data = mask_img.get_fdata()\n",
    "            hf.create_dataset('Lesion_mask', data=mask_data, compression=\"gzip\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {subject_id}: {e}\")\n",
    "\n",
    "print(\"\\nDataset creation complete.\")\n",
    "print(f\"HDF5 files are saved in: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60bf93c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从 '../../metadata/participants.tsv' 中读取到 1061 个受试者。\n",
      "在 '../../data/2_final_h5' 目录中找到 1030 个HDF5文件。\n",
      "元数据和实际数据取交集后，用于划分的受试者总数为: 1030\n",
      "成功创建并更新了 '../../metadata/splits.json'，包含 5 折的分层划分。\n",
      "每个折都包含了该折对应的验证集受试者ID。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "\n",
    "# 定义文件路径\n",
    "participants_path = '../../metadata/participants.tsv'\n",
    "splits_json_path = '../../metadata/splits.json'\n",
    "h5_dir = '../../data/2_final_h5'\n",
    "\n",
    "# 1. 读取并准备数据\n",
    "try:\n",
    "    df_metadata = pd.read_csv(participants_path, sep='\\t')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"无法找到文件: {participants_path}\")\n",
    "\n",
    "# 确保关键列存在\n",
    "required_cols = ['subject_id', 'center', 'Pathology']\n",
    "if not all(col in df_metadata.columns for col in required_cols):\n",
    "    raise ValueError(f\"'{participants_path}' 文件中缺少以下一列或多列: {required_cols}\")\n",
    "\n",
    "try:\n",
    "    # 从 .h5 文件名中提取 subject_id (例如 'ID123.h5' -> 'ID123')\n",
    "    h5_subject_ids = {f.replace('.h5', '') for f in os.listdir(h5_dir) if f.endswith('.h5')}\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"无法找到HDF5数据目录: {h5_dir}。请先运行第一个单元格生成HDF5文件。\")\n",
    "\n",
    "print(f\"从 '{participants_path}' 中读取到 {len(df_metadata)} 个受试者。\")\n",
    "print(f\"在 '{h5_dir}' 目录中找到 {len(h5_subject_ids)} 个HDF5文件。\")\n",
    "\n",
    "# 筛选出那些既在TSV文件中有记录，又存在对应HDF5文件的受试者\n",
    "df_filtered = df_metadata[df_metadata['subject_id'].isin(h5_subject_ids)].copy()\n",
    "print(f\"元数据和实际数据取交集后，用于划分的受试者总数为: {len(df_filtered)}\")\n",
    "\n",
    "if len(df_filtered) == 0:\n",
    "    raise ValueError(\"没有找到任何共有的受试者ID，无法进行划分。请检查ID格式是否一致。\")\n",
    "\n",
    "# 2. 创建分层标签和清理ID\n",
    "df_filtered['strata_group'] = df_filtered['center'] + '_' + df_filtered['Pathology']\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# 使用筛选后的数据进行划分\n",
    "X = df_filtered['subject_id']\n",
    "y = df_filtered['strata_group']\n",
    "\n",
    "splits_data = []\n",
    "\n",
    "# 直接迭代生成器\n",
    "for fold_idx, (_, val_idx) in enumerate(skf.split(X, y)):\n",
    "    val_subjects = X.iloc[val_idx].tolist()\n",
    "    \n",
    "    splits_data.append({\n",
    "        \"fold\": fold_idx,\n",
    "        \"val\": val_subjects\n",
    "    })\n",
    "\n",
    "# 4. 将结果写入JSON文件\n",
    "with open(splits_json_path, 'w') as f:\n",
    "    json.dump(splits_data, f, indent=2)\n",
    "\n",
    "print(f\"成功创建并更新了 '{splits_json_path}'，包含 {n_splits} 折的分层划分。\")\n",
    "print(\"每个折都包含了该折对应的验证集受试者ID。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9dc702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of strata groups across folds (validation sets):\n",
      "      AKH_ADC  Neimeng_ADC  AKH_SCC  Neimeng_SCC  AKH_SCLC  Neimeng_SCLC\n",
      "fold                                                                    \n",
      "0          68           47       40           26        16             9\n",
      "1          68           47       40           26        16             9\n",
      "2          68           46       40           27        16             9\n",
      "3          68           47       40           27        15             9\n",
      "4          68           47       39           26        16            10\n"
     ]
    }
   ],
   "source": [
    "# 修正后的统计单元格代码\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 读取 participants.tsv 和 splits.json\n",
    "participants_path = '../../metadata/participants.tsv'\n",
    "splits_json_path = '../../metadata/splits.json'\n",
    "\n",
    "df_participants = pd.read_csv(participants_path, sep='\\t')\n",
    "df_participants['strata_group'] = df_participants['center'] + '_' + df_participants['Pathology']\n",
    "\n",
    "with open(splits_json_path, 'r') as f:\n",
    "    splits_data = json.load(f)\n",
    "\n",
    "# 准备一个 DataFrame 来存储统计结果\n",
    "stats_list = []\n",
    "\n",
    "# 遍历每个 fold\n",
    "for fold_info in splits_data:\n",
    "    fold_num = fold_info['fold']\n",
    "    val_subjects = fold_info['val'] # ID不含'sub-'前缀\n",
    "    \n",
    "    # 直接使用原始的 'subject_id' 列进行匹配，因为两边都没有前缀\n",
    "    val_df = df_participants[df_participants['subject_id'].isin(val_subjects)]\n",
    "    \n",
    "    # 统计每个类别的数量\n",
    "    counts = val_df['strata_group'].value_counts().to_dict()\n",
    "    \n",
    "    # 添加 fold 编号并记录\n",
    "    counts['fold'] = fold_num\n",
    "    stats_list.append(counts)\n",
    "\n",
    "# 将统计结果转换为 DataFrame 以便更好地显示\n",
    "stats_df = pd.DataFrame(stats_list)\n",
    "stats_df = stats_df.set_index('fold')\n",
    "\n",
    "# # 填充 NaN 为 0，并确保所有类别都作为列存在\n",
    "# all_strata_groups = df_participants['strata_group'].unique()\n",
    "# for group in all_strata_groups:\n",
    "#     if group not in stats_df.columns:\n",
    "#         stats_df[group] = 0\n",
    "\n",
    "# stats_df = stats_df.fillna(0).astype(int)\n",
    "\n",
    "# # 重新排列列的顺序以便查看\n",
    "# stats_df = stats_df[sorted(all_strata_groups)]\n",
    "\n",
    "print(\"Distribution of strata groups across folds (validation sets):\")\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35713a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "totalseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
